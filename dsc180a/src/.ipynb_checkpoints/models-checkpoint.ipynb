{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bd3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "from model_classes_ import GCN_classification, GCN_regression, MLP_Classification, MLP_Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0868b2-66bf-455f-8380-9eb42b18517b",
   "metadata": {},
   "source": [
    "# Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21fc557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_classification(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer, name='GCN'):\n",
    "    '''\n",
    "    Trains model for classification task\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    max_epoch [int]: Maximum number of training epochs\n",
    "    learning_rate [float]: Learning rate\n",
    "    targetNode_mask [tensor]: Subgraph mask for training nodes\n",
    "    train_idx [array]: Node IDs corresponding to training set\n",
    "    valid_idx [array]: Node IDs corresponding to validation set\n",
    "    optimizer [PyTorch optimizer class]: PyTorch optimization algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_vec [array]: Training loss for each epoch\n",
    "    train_AUROC_vec [array]: Training AUROC score for each epoch\n",
    "    valid_loss_vec [array]: Validation loss for each epoch\n",
    "    valid_AUROC_vec [array]: Validation AUROC score for each epoch\n",
    "\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "    valid_loss_list = []\n",
    "    valid_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "\n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "    for e in list(range(max_epoch)):\n",
    "        \n",
    "        if e%100 == 0:\n",
    "            print(\"Epoch\", str(e), 'out of', str(max_epoch))\n",
    "        \n",
    "        model.train()\n",
    "        train_status = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Only trains on nodes with genes due to masking\n",
    "        if name == 'GCN':\n",
    "            forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "            train_scores = forward_scores[train_idx]\n",
    "        elif name == 'MLP':\n",
    "            forward_scores = model(graph.x.float())[targetNode_mask]\n",
    "            train_scores = forward_scores[train_idx].squeeze()        \n",
    "\n",
    "        train_loss  = model.loss(train_scores, torch.LongTensor(train_labels).to(device))\n",
    "\n",
    "        train_softmax, _ = model.calc_softmax_pred(train_scores)\n",
    "\n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        ### Calculate training and validation loss, AUROC scores\n",
    "        model.eval()\n",
    "        \n",
    "        valid_scores = forward_scores[valid_idx]\n",
    "        valid_loss  = model.loss(valid_scores, torch.LongTensor(valid_labels).to(device))\n",
    "        valid_softmax, _ = model.calc_softmax_pred(valid_scores) \n",
    "\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        train_softmax = to_cpu_npy(train_softmax)\n",
    "        train_AUROC = roc_auc_score(train_labels, train_softmax[:,1], average=\"micro\")\n",
    "\n",
    "        valid_loss_list.append(valid_loss.item())\n",
    "        valid_softmax = to_cpu_npy(valid_softmax)\n",
    "        valid_AUROC = roc_auc_score(valid_labels, valid_softmax[:,1], average=\"micro\")\n",
    "        \n",
    "        train_AUROC_vec[e] = train_AUROC\n",
    "        valid_AUROC_vec[e] = valid_AUROC\n",
    "\n",
    "    train_loss_vec = np.reshape(np.array(train_loss_list), (-1, 1))\n",
    "    valid_loss_vec = np.reshape(np.array(valid_loss_list), (-1, 1))\n",
    "\n",
    "    return train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec\n",
    "\n",
    "\n",
    "def train_model_regression(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer, name='GCN'):\n",
    "    '''\n",
    "    Trains model for regression task\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    max_epoch [int]: Maximum number of training epochs\n",
    "    learning_rate [float]: Learning rate\n",
    "    targetNode_mask [tensor]: Subgraph mask for training nodes\n",
    "    train_idx [array]: Node IDs corresponding to training set\n",
    "    valid_idx [array]: Node IDs corresponding to validation set\n",
    "    optimizer [PyTorch optimizer class]: PyTorch optimization algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_vec [array]: Training loss for each epoch;\n",
    "        analagous for valid_loss_vec (validation set)\n",
    "    train_pearson_vec [array]: Training PCC for each epoch;\n",
    "        analogous for valid_pearson_vec (validation set)\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_pearson_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "    valid_loss_list = []\n",
    "    valid_pearson_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "\n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "    for e in list(range(max_epoch)):\n",
    "        \n",
    "        if e%100 == 0:\n",
    "            print(\"Epoch\", str(e), 'out of', str(max_epoch))\n",
    "        \n",
    "        model.train()\n",
    "        train_status = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Only trains on nodes with genes due to masking\n",
    "        if name == 'GCN':\n",
    "            forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "            train_scores = forward_scores[train_idx]\n",
    "        elif name == 'MLP':\n",
    "            forward_scores = model(graph.x.float())[targetNode_mask]\n",
    "            train_scores = forward_scores[train_idx].squeeze()\n",
    "        \n",
    "        \n",
    "\n",
    "        train_loss  = model.loss(train_scores, torch.FloatTensor(train_labels).to(device))\n",
    "\n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        ### Calculate training and validation loss, AUROC scores\n",
    "        model.eval()\n",
    "        \n",
    "        train_scores = to_cpu_npy(train_scores)\n",
    "        train_pearson = calc_pearson(train_scores, train_labels)\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        \n",
    "        valid_scores = forward_scores[valid_idx]\n",
    "        valid_loss  = model.loss(valid_scores, torch.FloatTensor(valid_labels).to(device))\n",
    "        valid_scores = to_cpu_npy(valid_scores)\n",
    "        valid_pearson  = calc_pearson(valid_scores, valid_labels)\n",
    "        valid_loss_list.append(valid_loss.item())\n",
    "        \n",
    "        train_pearson_vec[e] = train_pearson\n",
    "        valid_pearson_vec[e] = valid_pearson\n",
    "\n",
    "    train_loss_vec = np.reshape(np.array(train_loss_list), (-1, 1))\n",
    "    valid_loss_vec = np.reshape(np.array(valid_loss_list), (-1, 1))\n",
    "\n",
    "    return train_loss_vec, train_pearson_vec, valid_loss_vec, valid_pearson_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32296307-19f7-46bd-8947-720bc505e63b",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d3b1f8-3b8a-4316-8e55-4b1bcc8a3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_classification(model, graph, targetNode_mask, train_idx, valid_idx, test_idx, name='GCN'):\n",
    "    '''\n",
    "    Runs fully trained classification model and compute evaluation statistics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    targetNode_mask [tensor]: Mask ensuring model only trains on nodes with genes\n",
    "    train_idx [array]: Node IDs corresponding to training set;\n",
    "        analogous for valid_idx and test_idx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_AUROC [float]: Test set AUROC score;\n",
    "        analogous for train_AUROC (training set) and valid_AUPR (validation set)\n",
    "    test_AUPR [float]: Test set AUPR score\n",
    "        analogous for train_AUPR (training set) and valid_AUPR (validation set)\n",
    "    test_pred [array]: Test set predictions;\n",
    "        analogous for train_pred (training set) and valid_pred (validation set)\n",
    "    test_labels [array]: Test set labels;\n",
    "        analagous for train_labels (training set) and valid_labels (validation set)\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    \n",
    "    model.eval()\n",
    "    train_status=False\n",
    "\n",
    "    if name == 'GCN':\n",
    "        forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "    elif name == 'MLP':\n",
    "        forward_scores = model(graph.x.float())[targetNode_mask]\n",
    "    \n",
    "    test_scores = forward_scores[test_idx]\n",
    "    \n",
    "    test_softmax, test_pred = model.calc_softmax_pred(test_scores) \n",
    "    \n",
    "    test_softmax = to_cpu_npy(test_softmax)\n",
    "    test_pred = to_cpu_npy(test_pred)\n",
    "    test_AUROC = roc_auc_score(test_labels, test_softmax[:,1], average=\"micro\")\n",
    "    test_precision, test_recall, thresholds = precision_recall_curve(test_labels, test_softmax[:,1])\n",
    "    test_AUPR = auc(test_recall, test_precision)\n",
    "    # test_F1 = f1_score(test_labels, test_pred, average=\"micro\")\n",
    "    \n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_softmax, train_pred = model.calc_softmax_pred(train_scores) \n",
    "    train_pred = to_cpu_npy(train_pred)\n",
    "    train_softmax = to_cpu_npy(train_softmax)\n",
    "    train_precision, train_recall, thresholds = precision_recall_curve(train_labels, train_softmax[:,1])\n",
    "    train_AUPR = auc(train_recall, train_precision)\n",
    "    # train_F1 = f1_score(train_labels, train_pred, average=\"micro\")\n",
    "\n",
    "    valid_scores = forward_scores[valid_idx]\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    valid_softmax, valid_pred = model.calc_softmax_pred(valid_scores) \n",
    "    valid_pred = to_cpu_npy(valid_pred)\n",
    "    valid_softmax = to_cpu_npy(valid_softmax)\n",
    "    valid_precision, valid_recall, thresholds = precision_recall_curve(valid_labels, valid_softmax[:,1])\n",
    "    valid_AUPR = auc(valid_recall, valid_precision)\n",
    "    # valid_F1 = f1_score(valid_labels, valid_pred, average=\"micro\")\n",
    "\n",
    "    return test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels\n",
    "\n",
    "\n",
    "def eval_model_regression(model, graph, targetNode_mask, train_idx, valid_idx, test_idx, name='GCN'):\n",
    "    '''\n",
    "    Runs fully trained regression model and compute evaluation statistics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    targetNode_mask [tensor]: Mask ensuring model only trains on nodes with genes\n",
    "    train_idx [array]: Node IDs corresponding to training set;\n",
    "        analogous for valid_idx and test_idx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_pearson [float]: PCC for test set;\n",
    "        analogous for train_pearson (training set) and valid_pearson (validation set)\n",
    "    test_pred [array]: Test set predictions;\n",
    "        analogous for train_pred (training set) and valid_pred (validation set)\n",
    "    test_labels [array]: Test set labels (expression values);\n",
    "        analagous for train_labels (training set) and valid_labels (validation set)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    train_status=False\n",
    "\n",
    "    if name == 'GCN':\n",
    "        forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "    elif name == 'MLP':\n",
    "        forward_scores = model(graph.x.float())[targetNode_mask]\n",
    "\n",
    "    test_scores = forward_scores[test_idx]\n",
    "    test_pred = to_cpu_npy(test_scores)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    test_pearson = calc_pearson(test_pred, test_labels)\n",
    "\n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_pred = to_cpu_npy(train_scores)\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_pearson = calc_pearson(train_pred, train_labels)\n",
    "\n",
    "    valid_scores = forward_scores[valid_idx]\n",
    "    valid_pred = to_cpu_npy(valid_scores)\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    valid_pearson = calc_pearson(valid_pred, valid_labels)\n",
    "\n",
    "    return test_pearson, test_pred, test_labels, train_pearson, train_pred, train_labels, \\\n",
    "        valid_pearson, valid_pred, valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96a190-83cf-4bb4-8f04-61933b4c3214",
   "metadata": {},
   "source": [
    "# Pearson score calculator + GPU to CPU tensor Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81dc3213-0461-4837-a8af-33e325009273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pearson(scores, targets):\n",
    "    '''\n",
    "    Calculates Pearson correlation coefficient (PCC) between predicted \\\n",
    "        expression levels and true expression levels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores [array]: Predicted expression levels\n",
    "    targets [array]: True expression levels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pcc [float]: Pearson correlation coefficient\n",
    "\n",
    "    '''\n",
    "    scores = scores.squeeze()\n",
    "    targets = targets.squeeze()\n",
    "    pcc, _ = pearsonr(scores, targets)\n",
    "            \n",
    "    return pcc\n",
    "    \n",
    "    \n",
    "def to_cpu_npy(x):\n",
    "    '''\n",
    "    Simple helper function to transfer GPU tensors to CPU numpy matrices\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x [tensor]: PyTorch tensor stored on GPU\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_x [array]: Numpy array stored on CPU\n",
    "\n",
    "    '''\n",
    "\n",
    "    new_x = x.cpu().detach().numpy()\n",
    "    \n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70354865",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc67fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains and evaluates models\n",
    "###Test for GPU availability\n",
    "cuda_flag = torch.cuda.is_available()\n",
    "if cuda_flag:  \n",
    "  dev = \"cuda\" \n",
    "else:\n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6bda395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(cell_line, regression_flag, model, max_epoch=50, learning_rate=0.001, name='GCN'):\n",
    "    \"\"\"\n",
    "    \n",
    "        Trains and evaluates models.\n",
    "        \n",
    "        Parameters:\n",
    "        cell_line: cell line to be trained on\n",
    "        regression_flag: 0 (Classification) or 1 (Regression)\n",
    "        model: GCN, MLP, or CNN model\n",
    "        name: GCN, MLP, CNN\n",
    "        \n",
    "        Returns:\n",
    "        Performance scores based on type of task\n",
    "        \n",
    "    \"\"\"\n",
    "    if regression_flag == 0:\n",
    "        num_classes = 2\n",
    "        task = 'Classification'\n",
    "    else:\n",
    "        num_classes = 1\n",
    "        task = 'Regression'\n",
    "\n",
    "    # random_seed = random.randint(0,10000)\n",
    "    # random.seed(random_seed)\n",
    "    # np.random.seed(random_seed)\n",
    "    # torch.manual_seed(random_seed)\n",
    "\n",
    "\n",
    "    ###Initialize start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    today = date.today()\n",
    "    mdy = today.strftime(\"%Y-%m-%d\")\n",
    "    clock = datetime.now()\n",
    "    hms = clock.strftime(\"%H-%M-%S\")\n",
    "    hm = clock.strftime(\"%Hh-%Mm\")\n",
    "    hm_colon = clock.strftime(\"%H:%M\")\n",
    "    date_and_time = mdy + '-at-' + hms\n",
    "    \n",
    "    ###Load input files\n",
    "    base_path = os.getcwd()\n",
    "    save_dir = os.path.join(base_path, 'data', cell_line, 'saved_runs')\n",
    "    hic_sparse_mat_file = os.path.join(base_path, 'data', cell_line, 'hic_sparse.npz')\n",
    "    np_nodes_lab_genes_file = os.path.join(base_path, 'data',  cell_line, \\\n",
    "        'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "    np_hmods_norm_all_file = os.path.join(base_path, 'data', cell_line, \\\n",
    "        'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "    df_genes_file = os.path.join(base_path, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "    df_genes = pd.read_pickle(df_genes_file)\n",
    "    \n",
    "    # Load data\n",
    "    mat = load_npz(hic_sparse_mat_file)\n",
    "    allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "    hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "    X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "    allNodes = allNodes_hms[:, 0].astype(int)\n",
    "    geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "    geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "    allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "    targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "    if regression_flag == 0:\n",
    "        geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "        allLabs[geneNodes] = geneLabs\n",
    "        Y = torch.tensor(allLabs).long()\n",
    "    else:\n",
    "        geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "        allLabs[geneNodes] = geneLabs\n",
    "        Y = torch.tensor(allLabs).float()\n",
    "\n",
    "    extract = torch_geometric.utils.from_scipy_sparse_matrix(mat)\n",
    "    data = torch_geometric.data.Data(edge_index = extract[0], edge_attr = extract[1], x = X, y = Y)\n",
    "    G = data\n",
    "    \n",
    "    # Randomize node order and split into 70%/15%/15% training/validation/test sets\n",
    "    pred_idx_shuff = torch.randperm(targetNode_mask.shape[0])\n",
    "\n",
    "    fin_train = np.floor(0.7*pred_idx_shuff.shape[0]).astype(int)\n",
    "    fin_valid = np.floor(0.85*pred_idx_shuff.shape[0]).astype(int)\n",
    "    train_idx = pred_idx_shuff[:fin_train]\n",
    "    valid_idx = pred_idx_shuff[fin_train:fin_valid]\n",
    "    test_idx = pred_idx_shuff[fin_valid:]\n",
    "\n",
    "    train_gene_ID = targetNode_mask[train_idx].numpy()\n",
    "    valid_gene_ID = targetNode_mask[valid_idx].numpy()\n",
    "    test_gene_ID = targetNode_mask[test_idx].numpy()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "    # print(\"\\n\"+\"Model's state_dict:\")\n",
    "    # for param_tensor in model.state_dict():\n",
    "    #     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    \n",
    "    ### For classification:\n",
    "    if regression_flag == 0:\n",
    "\n",
    "        ### Train model\n",
    "        train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec = \\\n",
    "            train_model_classification(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer, name=name)\n",
    "\n",
    "        ### Evaluate model\n",
    "        test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "                valid_AUPR, valid_pred, valid_labels = \\\n",
    "                    eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx, name=name)\n",
    "\n",
    "        ### Save metrics and node predictions\n",
    "        train_metrics = [train_gene_ID, train_pred, train_labels, train_AUROC_vec, train_AUPR, train_loss_vec]\n",
    "        # np.save(os.path.join(save_dir, 'model_' + date_and_time + '_train_metrics'  + '.npy'), train_metrics)\n",
    "\n",
    "        valid_metrics = [valid_gene_ID, valid_pred, valid_labels, valid_AUROC_vec, valid_AUPR, valid_loss_vec]\n",
    "        # np.save(os.path.join(save_dir, 'model_' + date_and_time + '_valid_metrics'  + '.npy'), valid_metrics)\n",
    "\n",
    "        test_metrics = [test_gene_ID, test_pred, test_labels, test_AUROC, test_AUPR, ['na']]\n",
    "        # np.save(os.path.join(save_dir, 'model_' + date_and_time + '_test_metrics'  + '.npy'), test_metrics)\n",
    "\n",
    "        dataset_list = [train_metrics, valid_metrics, test_metrics]\n",
    "        df_full_metrics = pd.DataFrame(columns=['Dataset','Node ID','True Label','Predicted Label','Classification'])\n",
    "\n",
    "        for d in np.arange(len(dataset_list)):\n",
    "            dataset_metrics = dataset_list[d]\n",
    "            partial_metrics = pd.DataFrame()\n",
    "\n",
    "            partial_metrics['Node ID'] = dataset_metrics[0]\n",
    "            partial_metrics['True Label'] = dataset_metrics[2]\n",
    "            partial_metrics['Predicted Label'] = dataset_metrics[1]\n",
    "            partial_metrics['Classification'] = dataset_metrics[1]*1 + dataset_metrics[2]*2\n",
    "            partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
    "            partial_metrics['Classification'].replace(to_replace=1, value='FP', inplace=True)\n",
    "            partial_metrics['Classification'].replace(to_replace=2, value='FN', inplace=True)\n",
    "            partial_metrics['Classification'].replace(to_replace=3, value='TP', inplace=True)\n",
    "\n",
    "            if d == 0:\n",
    "                partial_metrics['Dataset'] = 'Training'\n",
    "            elif d == 1:\n",
    "                partial_metrics['Dataset'] = 'Validation'\n",
    "            elif d == 2:\n",
    "                partial_metrics['Dataset'] = 'Testing'\n",
    "\n",
    "            df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n",
    "\n",
    "        df_gene_names = df_genes.iloc[:,:3]\n",
    "        df_gene_names = df_gene_names.rename(columns={\"gene_catalog_name\": \"ENSEMBL_ID\", \"abbrev\": \"Abbreviation\",\n",
    "                                      \"hic_node_id\" : 'Node ID'})\n",
    "        df_full_metrics = pd.merge(df_full_metrics, df_gene_names, how='inner', on='Node ID')\n",
    "        df_full_metrics = df_full_metrics[df_full_metrics.columns[[0,1,5,6,2,3,4]]]\n",
    "\n",
    "    ### For regression:\n",
    "    elif regression_flag == 1:\n",
    "\n",
    "        ### Train model\n",
    "        train_loss_vec, train_pearson_vec, valid_loss_vec, valid_pearson_vec = \\\n",
    "            train_model_regression(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer, name=name)\n",
    "\n",
    "        ### Evaluate model\n",
    "        test_pearson, test_pred, test_labels, train_pearson, train_pred, train_labels, \\\n",
    "                valid_pearson, valid_pred, valid_labels = \\\n",
    "                    eval_model_regression(model, G, targetNode_mask, train_idx, valid_idx, test_idx, name=name)\n",
    "\n",
    "        ### Save metrics and node predictions\n",
    "        train_metrics = [train_gene_ID, train_pred, train_labels, train_pearson_vec, train_loss_vec]\n",
    "        # np.save(os.path.join(save_dir, 'model_' + date_and_time + '_train_metrics'  + '.npy'), train_metrics)\n",
    "\n",
    "        valid_metrics = [valid_gene_ID, valid_pred, valid_labels, valid_pearson_vec, valid_loss_vec]\n",
    "        # np.save(os.path.join(save_dir, 'model_' + date_and_time + '_valid_metrics'  + '.npy'), valid_metrics)\n",
    "\n",
    "        test_metrics = [test_gene_ID, test_pred, test_labels, test_pearson, ['na']]\n",
    "        # np.save(os.path.join(save_dir, 'model_' + date_and_time + '_test_metrics'  + '.npy'), test_metrics)\n",
    "\n",
    "        dataset_list = [train_metrics, valid_metrics, test_metrics]\n",
    "        df_full_metrics = pd.DataFrame(columns=['Dataset','Node ID','True Label','Predicted Label'])\n",
    "\n",
    "        for d in np.arange(len(dataset_list)):\n",
    "            dataset_metrics = dataset_list[d]\n",
    "            partial_metrics = pd.DataFrame()\n",
    "\n",
    "            partial_metrics['Node ID'] = dataset_metrics[0]\n",
    "            partial_metrics['True Label'] = dataset_metrics[2]\n",
    "            partial_metrics['Predicted Label'] = dataset_metrics[1]\n",
    "\n",
    "            if d == 0:\n",
    "                partial_metrics['Dataset'] = 'Training'\n",
    "            elif d == 1:\n",
    "                partial_metrics['Dataset'] = 'Validation'\n",
    "            elif d == 2:\n",
    "                partial_metrics['Dataset'] = 'Testing'\n",
    "\n",
    "            df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n",
    "\n",
    "        df_gene_names = df_genes.iloc[:,:3]\n",
    "        df_gene_names = df_gene_names.rename(columns={\"gene_catalog_name\": \"ENSEMBL_ID\", \"abbrev\": \"Abbreviation\",\n",
    "                                      \"hic_node_id\" : 'Node ID'})\n",
    "        df_full_metrics = pd.merge(df_full_metrics, df_gene_names, how='inner', on='Node ID')\n",
    "        df_full_metrics = df_full_metrics[df_full_metrics.columns[[0,1,4,5,2,3]]]\n",
    "\n",
    "\n",
    "    ### Print elapsed time and performance\n",
    "    elapsed = (time.time() - start_time)\n",
    "    elapsed_h = int(elapsed//3600)\n",
    "    elapsed_m = int((elapsed - elapsed_h*3600)//60)\n",
    "    elapsed_s = int(elapsed - elapsed_h*3600 - elapsed_m*60)\n",
    "    print('Elapsed time: {0:02d}:{1:02d}:{2:02d}'.format(elapsed_h, elapsed_m, elapsed_s))\n",
    "\n",
    "    \n",
    "    print('Performance:')\n",
    "    if regression_flag == 0:\n",
    "        print('Test AUROC:', test_AUROC, '\\n')\n",
    "        return test_AUROC\n",
    "    \n",
    "    elif regression_flag == 1:\n",
    "        print('Test pearson:', test_pearson, '\\n')\n",
    "        return test_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30edcf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores AUROC, AUPR, and PCC across all models for each cell line\n",
    "data_dict = {}\n",
    "data_dict['E116'] = {'AUROC': [None, None, None], 'PCC': [None, None, None]}\n",
    "data_dict['E122'] = {'AUROC': [None, None, None], 'PCC': [None, None, None]}\n",
    "data_dict['E123'] = {'AUROC': [None, None, None], 'PCC': [None, None, None]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b91ba7",
   "metadata": {},
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b23c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set random seed\n",
    "# random_seed = random.randint(0,10000)\n",
    "# random.seed(random_seed)\n",
    "# np.random.seed(random_seed)\n",
    "# torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2816c72e-ef9c-4557-8a2c-2e609ba53b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "regression_flag = 0\n",
    "max_epoch = 50\n",
    "learning_rate = 0.001\n",
    "num_lin_layers = 2\n",
    "lin_hidden_size = 100\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_runs = 10\n",
    "graph_conv_layer_sizes = [num_feat] + \\\n",
    "        [int(max(graph_conv_embed_size, lin_hidden_size)) \\\n",
    "              for i in np.arange(1, num_graph_conv_layers, 1)] + [lin_hidden_size]\n",
    "\n",
    "lin_hidden_sizes_r = [graph_conv_layer_sizes[-1]] + \\\n",
    "        [int(max(lin_hidden_size, 1)) \\\n",
    "              for i in np.arange(1, num_lin_layers, 1)] + [1]\n",
    "lin_hidden_sizes_c = [graph_conv_layer_sizes[-1]] + \\\n",
    "        [int(max(lin_hidden_size, 2)) \\\n",
    "              for i in np.arange(1, num_lin_layers, 1)] + [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e13c30-ace4-46c6-8014-1f313afae8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_n(cell_line, num_runs, model_c, model_r, name, max_epoch=50, models={'MLP': 0, 'GCN': 1, 'CNN': 2}):\n",
    "    \"\"\"\n",
    "        Trains and evaluates the models over a certain number of runs.\n",
    "        This function calculates average and standard deviation values\n",
    "        and populates fields in the data_dict dictionary according for\n",
    "        each model.\n",
    "\n",
    "        Parameters:\n",
    "            num_runs: number of times to execute the experiment\n",
    "            model_c: classification model\n",
    "            model_r: regression model\n",
    "            name: MLP, GCN, or CNN\n",
    "            models: maps index to position in data_dict\n",
    "            \n",
    "    \"\"\"\n",
    "    aurocs = []\n",
    "    pccs = []\n",
    "\n",
    "    print('Classification Task...')\n",
    "    for i in range(num_runs):\n",
    "        print(f\"Iteration {i+1}:\")\n",
    "        # MLP Classification Task\n",
    "        auroc = experiment(cell_line=cell_line, regression_flag=0, max_epoch=max_epoch, learning_rate=learning_rate, model=model_c, name=name)\n",
    "        aurocs.append(auroc)\n",
    "\n",
    "    print('Regression Task...')\n",
    "    for i in range(num_runs):\n",
    "        print(f\"Iteration {i+1}:\")\n",
    "        # MLP Regression Task\n",
    "        pearson = experiment(cell_line=cell_line, regression_flag=1, max_epoch=max_epoch+25, learning_rate=learning_rate, model=model_r, name=name)\n",
    "        pccs.append(pearson)\n",
    "\n",
    "    aurocs, pccs = np.array(aurocs), np.array(pccs)\n",
    "    data_dict[cell_line]['AUROC'][models[name]] = (np.mean(aurocs), np.std(aurocs))\n",
    "    data_dict[cell_line]['PCC'][models[name]] = (np.mean(pccs), np.std(pccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e815f84-abed-4173-bd59-7cda4b3a4c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell line: E116\n",
      "Iteration 1:\n",
      "\n",
      "\n",
      "Epoch 0 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:02\n",
      "Performance:\n",
      "Test AUROC: 0.8787298366487437 \n",
      "\n",
      "\n",
      "\n",
      "Epoch 0 out of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2505])) that is different to the input size (torch.Size([2505, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_4753/359630346.py:188: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:23\n",
      "Performance:\n",
      "Test pearson: 0.7208783290772405 \n",
      "\n",
      "Iteration 2:\n",
      "\n",
      "\n",
      "Epoch 0 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:01\n",
      "Performance:\n",
      "Test AUROC: 0.8922110225189873 \n",
      "\n",
      "\n",
      "\n",
      "Epoch 0 out of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2505])) that is different to the input size (torch.Size([2505, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_4753/359630346.py:188: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:23\n",
      "Performance:\n",
      "Test pearson: 0.7652887061206204 \n",
      "\n",
      "Iteration 3:\n",
      "\n",
      "\n",
      "Epoch 0 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:01\n",
      "Performance:\n",
      "Test AUROC: 0.9040093373056707 \n",
      "\n",
      "\n",
      "\n",
      "Epoch 0 out of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2505])) that is different to the input size (torch.Size([2505, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_4753/359630346.py:188: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:23\n",
      "Performance:\n",
      "Test pearson: 0.7633084068852359 \n",
      "\n",
      "Iteration 4:\n",
      "\n",
      "\n",
      "Epoch 0 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:01\n",
      "Performance:\n",
      "Test AUROC: 0.9053224547766324 \n",
      "\n",
      "\n",
      "\n",
      "Epoch 0 out of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2505])) that is different to the input size (torch.Size([2505, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_4753/359630346.py:188: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:23\n",
      "Performance:\n",
      "Test pearson: 0.7722064822851963 \n",
      "\n",
      "Iteration 5:\n",
      "\n",
      "\n",
      "Epoch 0 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:01\n",
      "Performance:\n",
      "Test AUROC: 0.9177687688689046 \n",
      "\n",
      "\n",
      "\n",
      "Epoch 0 out of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2505])) that is different to the input size (torch.Size([2505, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_4753/359630346.py:188: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:23\n",
      "Performance:\n",
      "Test pearson: 0.7655699523721421 \n",
      "\n",
      "Iteration 6:\n",
      "\n",
      "\n",
      "Epoch 0 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:01\n",
      "Performance:\n",
      "Test AUROC: 0.9038809403716115 \n",
      "\n",
      "\n",
      "\n",
      "Epoch 0 out of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2505])) that is different to the input size (torch.Size([2505, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_4753/359630346.py:188: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:23\n",
      "Performance:\n",
      "Test pearson: 0.7945035506153113 \n",
      "\n",
      "Iteration 7:\n",
      "\n",
      "\n",
      "Epoch 0 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:01\n",
      "Performance:\n",
      "Test AUROC: 0.9027040617866539 \n",
      "\n",
      "\n",
      "\n",
      "Epoch 0 out of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2505])) that is different to the input size (torch.Size([2505, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/tmp/ipykernel_4753/359630346.py:188: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_full_metrics = pd.concat([df_full_metrics, partial_metrics], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:23\n",
      "Performance:\n",
      "Test pearson: 0.7915630110014131 \n",
      "\n",
      "Iteration 8:\n",
      "\n",
      "\n",
      "Epoch 0 out of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n",
      "/tmp/ipykernel_4753/359630346.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  partial_metrics['Classification'].replace(to_replace=0, value='TN', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 00:00:01\n",
      "Performance:\n",
      "Test AUROC: 0.9075054640209799 \n",
      "\n",
      "\n",
      "\n",
      "Epoch 0 out of 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([2505])) that is different to the input size (torch.Size([2505, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "for cell_line in data_dict:\n",
    "        print(f\"Cell line: {cell_line}\")\n",
    "        experiment_n(\n",
    "            cell_line, \n",
    "            num_runs, \n",
    "            MLP_Classification(num_feat, lin_hidden_sizes_c, 2),\n",
    "            MLP_Regression(num_feat, lin_hidden_sizes_r, 1),\n",
    "            'MLP', \n",
    "            max_epoch=max_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bfeb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64192ec-a0bb-4673-9a0c-96a35e4cace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd5688",
   "metadata": {},
   "source": [
    "## GC-Merge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f44a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cell_line in data_dict:\n",
    "        print(f\"Cell line: {cell_line}\")\n",
    "        experiment_n(\n",
    "            cell_line, \n",
    "            num_runs, \n",
    "            GCN_classification(num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, lin_hidden_sizes_c, 2),\n",
    "            GCN_regression(num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, lin_hidden_sizes_r, 1),\n",
    "            'GCN', \n",
    "            max_epoch=max_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff3f9fe",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df38e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "\n",
    "random_seed = random.randint(0,10000)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# cell_line = ...\n",
    "# regression_flag = ...\n",
    "# max_epoch = ...\n",
    "# learning_rate = ...\n",
    "# num_graph_conv_layers = ...\n",
    "# graph_conv_embed_size = ...\n",
    "# num_lin_layers = ...\n",
    "# lin_hidden_size = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "\n",
    "NUM_SAMPLES = 100\n",
    "INPUT_LENGTH = 6   # Length of each input sequence\n",
    "NUM_CLASSES = 2    # Binary classification\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "base_path = os.getcwd()\n",
    "data_file = os.path.join(base_path, 'data', cell_line, 'np_hmods_norm_chip_10000bp.npy')\n",
    "input_data = np.load(data_file)\n",
    "\n",
    "target_labels = torch.randint(0, 2, (NUM_SAMPLES,))\n",
    "\n",
    "output = model(input_data)\n",
    "loss = criterion(output, target_labels)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "dataset = TensorDataset(input_data, target_labels)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_inputs, batch_labels in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch_inputs)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357747ce-c391-4822-b1e5-f93645b5b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(cell_line='E116', regression_flag=regression_flag, learning_rate=learning_rate, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89142988",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e918e9c-c077-49a8-ace9-c1850083b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the cell lines and their corresponding data\n",
    "cell_lines = list(data_dict.keys())\n",
    "categories = cell_lines\n",
    "values = [[v[0] for v in data_dict[cell_lines[0]]], [v[0] for v in data_dict[cell_lines[1]]], [v[0] for v in data_dict[cell_lines[2]]]]\n",
    "values\n",
    "\n",
    "AUROC_MLP = [data_dict[line][0][0] for line in cell_lines]\n",
    "PCC_MLP = [data_dict[line][0][2] for line in cell_lines]\n",
    "\n",
    "AUROC_GCN = [data_dict[line][1][0] for line in cell_lines]\n",
    "PCC_GCN = [data_dict[line][1][2] for line in cell_lines]\n",
    "\n",
    "# AUROC graphs\n",
    "X_axis = np.arange(0,len(categories))\n",
    "plt.bar(X_axis + 0.2, AUROC_MLP, 0.4, label='MLP')\n",
    "plt.bar(X_axis-0.2, AUROC_GCN, 0.4, label='GCN')\n",
    "plt.xticks(X_axis, categories)\n",
    "plt.xlabel(\"Cell lines\")\n",
    "plt.ylabel(\"AUROC\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# PCC Graphs\n",
    "plt.bar(X_axis + 0.2, PCC_MLP, 0.4, label='MLP')\n",
    "plt.bar(X_axis-0.2, PCC_GCN, 0.4, label='GCN')\n",
    "plt.xticks(X_axis, categories)\n",
    "plt.xlabel(\"Cell lines\")\n",
    "plt.ylabel(\"PCC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121179d2-6202-4fa3-ba15-fec13602bee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
